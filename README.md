# Leveraging Computer Vision for Real-Time Natural Diasaster Analysis 

This repository contains satellite images, helper files, and test files to build a models that can classify natural disaster images based on the type of natural disaster and the level of damage. 

## Background

This project involves developing a computer vision approach to assist a crisis response agency in analyzing satellite imagery for disaster management. The goal is to automate disaster type and damage level classification using data from the xView2 Challenge Dataset, which contains images labeled by damage levels (0: no damage to 3: destroyed) across various disasters (midwest-flooding, socal-fire, hurricane-matthew). The project includes exploratory data analysis (EDA) to understand dataset characteristics and extract useful features, followed by two tasks: 

- (A) classifying disaster types (midwest-flooding vs. socal-fire) 
- (B) classifying building damage levels for hurricane-matthew. 

## Local SetUp

To set up your local environment, run `make environment` from the command line. This will load the necessary libraries and modules with Python 3.10. 

This repository does not include the images provided for the project (for reproducability reasons). However, those images were a subset of the xView2 Challenge Dataset, and can be found from that website. Save the images into this directory, because the CNN needs to access the images and load them directly.

## EDA and Features Analyzed

We turned the data into a structured format by converting the list of images to an image size, and also converting the images into red, green, and blue channels for better comparison. We also looked at a variety of other parameters to understand our data. In addition to average RGB value, we looked at Local Binary Patterns (LBP) values, Sobel edge intensity, and Gabor filter responses. LBP is a texture descriptor that encodes pixel intensity patterns into binary numbers. The Gabor kernel is a filter that detects edges. Sobel edge filtering uses convolution to detect edges by calculating intensity gradients in an image. and textures by responding to specific frequencies and orientations. 

Through these analyses, we found key distinctions between disaster types and hurricane damage levels. For instance, images from SoCal Fires exhibit higher red and green RGB values, as well as elevated Sobel edge and Gabor filter responses, suggesting sharper edges and more intricate textures. Midwest Flood images, by comparison, showed less saturation and edge intensity. Within the Hurricane Matthew dataset, severely damaged buildings (Level 3) show lower red and green values, reduced edge intensities, and increased LBP values, reflecting diminished visibility and simpler textures compared to less damaged structures (Level 1). Additionally, we found that log transformations of the color channel values were necessary to create an even distribution of the data. For values of Gabor Kernel, Sobel Edge Filtering, and Local Binary Patterns, the distributions were approximately Gaussian, so we do not need such transformations. 

**Based on these findings, we selected red and green RGB channels, LBP, Sobel edge intensity, and Gabor filter response as predictor variables for the modeling phase.**

## Files Contained

### `utility_scripts`

The `data_utils.py` contains functions to load the dataset and visualize it. The `feature_utils.py` contains a handful of functions to define useful features from the loaded images. Both of these files were given by the Data 200 staff. The `functions.py` contains functions that we have written for our feature engineering, such as those to calculate the LBP, Gabor Kernel, and Sobel Edges. The `feature_engineer.py` contains the functions we used to clean the dataset, such as calculating the image size and generating a `.csv` file for each dataset with the features. At the end of the data cleaning, csv files of the dataframes are generated to pss into the modeling notebooks smoothly. The `config.json` was provided by the Data 200 staff, and it used to load in the data. 

### `test_csv`

This folder contains all of the predictions made on the test sets provided by Data 200 staff. They represent predictions done with various modeling efforts along the way. They are named by the dataset they are predicting and any key features of the model. 

### `feature_eng_csv`

This folder contains `.csv` files that were feature engineered from the scripts in `utility_scripts`. Each file contains the features extracted from each image and the corresponding label. In the `socal_fires_midwest_floods.csv` file, the labels are `0` for flood data and `1` for fire data. There exist 4 Hurricane Matthew datasets. Two were generated by one hot encoding the target variable, removing any influence of the magnitude of the class label on the dataset. The sampling methods also differ. In the unbalanced files, all of the data points are included, even if that creates class imbalance. In the balanced `.csv` files, the labels with more corresponding images are randomly sampled to equal the label with the smallest number of edges in occurence. 

### `task_A_dep`

This folder contains deprecated Jupyter notebooks containing models that were tried previously with less success. For task A, this includes trying a different CNN architecture and different principal components for PCA. 

### `task_B_dep`

This folder contains deprecated Jupyter notebooks containing models that were tried previously on the Hurricane Matthew dataset. This includes different principal components for PCA and a one vs rest approach to predicting multinomial data. 

## Final Modeling Notebooks 

The modeling work is split up into two notebooks; one for each task. `task_A_logreg_final.ipynb` contains both logistic regression models and the subsequent test set generation code. The first model is a vanilla logistic regression model with no frills added. The second is a logistic regression model with gradient descent, PCA with 3 principal components, and hyperparameter tuning implemented to augment the results. 

The same models are included in `task_B_logreg_final.ipynb`, and they are modified with focal loss function to better fit the multinomial prediction task. 

`task_A_modeling_CNN.ipynb` and `task_B_modeling_CNN.ipynb` both include different CNN architectures to analyze the images directly. The images are resized upon loading to reduce computational cost, and the models are trained with different hyperparameters to achieve desirable levels of accuracy on training and validation data. 

## Results and Implications

The Vanilla Logistic Regression model performed effectively on the Fire/Flood dataset, achieving an F1 score of 0.91. Applying PCA, hyperparameter tuning, and k-fold cross-validation significantly improved the logistic regression model's performance on the Fire/Flood dataset, achieving an F1 score of 0.92. However, it struggled with the Hurricane Matthew dataset, failing to converge and resulting in poor accuracy scores of around 25% for training and validation. 

Efforts to improve performance on this dataset with both models, including truncating the data, yielded only marginal gains, with accuracy improving to 0.46 and 0.44, respectively. However, these enhancements proved less effective for the Hurricane Matthew dataset, which continued to experience challenges due to class imbalance and overlap in feature distributions, even after synthetic data augmentation and other mitigation strategies.  

The CNN demonstrated superior performance on the Fire/Flood dataset, achieving over 97% accuracy and F1 scores with minimal loss. This reflects the CNN’s ability to capture hierarchical and nonlinear features from image data. For the Hurricane Matthew dataset, despite efforts like increasing dropout for regularization and adding hidden layers, the CNN achieved only moderate success, with training accuracy of 70% and validation accuracy capping at 58%. Attempts at further data augmentation, including SMOTE, did not yield improvements. This higlights the dataset's complexity and the limitations of these techniques in this context.  
 
The CNN model's ability to learn features directly from raw image data shows its potential for handling complex, nonlinear relationships compared to logistic regression models, which rely on engineered features and PCA for dimensionality reduction. However, challenges such as computational expense and the necessity of large datasets hinder the model’s scalability. While logistic regression models are less computationally intensive, they struggle with datasets like Hurricane Matthew's, where class overlap and imbalance degrades performance. 

### Real World Implications
The successful application of CNNs in disaster classification has significant implications for real-world scenarios. Their ability to rapidly process large datasets can aid in disaster response by identifying and prioritizing affected areas, enabling timely allocation of resources. However, the reliance on CNNs comes with risks, such as potential misclassifications and the "black-box" nature of predictions, which may reduce trust among stakeholders. Integrating explainability tools like Grad-CAM can mitigate these concerns by providing insights into the decision-making process, ensuring that CNNs are both effective and trustworthy in high-stakes environments.

### Developed by Marie Anand, Shree Patel, and Sophia Zhang
